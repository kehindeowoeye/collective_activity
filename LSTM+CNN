from keras.models import Sequential
import keras
from keras.layers import Dense
from keras.layers import TimeDistributed
from keras.layers import LSTM,RepeatVector
from keras.models import load_model
import json
from keras.models import model_from_json, load_model
from keras.layers.normalization import BatchNormalization
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D
from keras.layers.core import Activation
from keras.layers.core import Flatten
from keras.layers.core import Dropout
from keras import backend as K
from keras.layers import Merge, Dense
from keras.layers import Reshape
from keras.layers import Input, Embedding, LSTM, Dense
from keras.models import Model
from keras.utils import to_categorical
import random
import math
import xlrd
import xlsxwriter
import pandas as pd
import numpy as np
from keras.callbacks import EarlyStopping
from keras.callbacks import ModelCheckpoint
from keras.models import model_from_json
from keras.models import load_model
from sklearn.metrics import classification_report, confusion_matrix
from keras.callbacks import Callback

class WeightsSaver(Callback):
    def __init__(self, N):
        self.N = N
        self.batch = 0
    
    def on_batch_end(self, batch, logs={}):
        if self.batch % self.N == 0:
            name = 'weights%08d.h5' % self.batch
            self.model.save_weights(name)
        self.batch += 1



early_stopping = EarlyStopping(monitor='val_loss', patience=0)
dim_output = 1

n_neurons = 30
num_features = 72
look_back = 30


dataset = np.array(pd.read_excel('alldata0404.xls',header=None))
labels = np.array(dataset[:,72])
lab = labels[1:len(labels)-look_back-1]
labl = np.array(pd.get_dummies(lab))
#labels = labels.reshape(labels.shape[0],1)



dim_output = 2

n_epoch = 50
n_batch = 10

dd = dataset.shape
trainl = math.trunc(1.0*dd[0])
test = dd[0]-trainl


    


for x in range(0, 1):
     dataset0401x =  dataset[:,0:36];
     #dataset0401x =np.delete( dataset0401x,13,axis=1)
     print(dataset0401x.shape)
     dataset0401y =  dataset[:,36:72];
     #dataset0401y =np.delete(dataset0401y,13,axis=1)
     acx = np.diff(dataset0401x,axis = 0);acy = np.diff(dataset0401y,axis = 0)
     zz =  np.sqrt(np.power(acx,2) + np.power(acy,2))
     #rx1 = dataset0401x[:,x]; dataset0401x = np.delete(dataset0401x,(x),axis = 1)
     #ry1 = dataset0401y[:,x]; dataset0401y = np.delete(dataset0401y,(x),axis = 1)
     rx = np.mean(dataset0401x,axis=1); ry = np.mean(dataset0401y,axis=1);
     rx = np.array([rx])
     rx = rx.reshape(rx.shape[1],1)
     ry = np.array([ry])
     ry = ry.reshape(ry.shape[1],1)
     rx = np.array(dataset0401x)-np.array(rx)
    
     ry = np.array(dataset0401y)-np.array(ry)
     dist = np.sqrt(np.power(rx,2)+ np.power(ry,2))        
     dist = np.sort(np.array(dist), axis=1)
     dist = np.array(dist[0:trainl-1,:]);
     #dist = zz
     dist = np.concatenate((dist,zz),axis = 1)      #*changes
     Xtrain = np.array(dist[0:trainl,:]);Xtest = np.array(dist[trainl:,:]);
     labeltrain = np.array(labels[0:trainl],object);laca = labeltrain
     labeltest = np.array(labels[trainl:]);
      
    
nb_samples = Xtrain.shape[0] - look_back
x_train_reshaped1 = np.zeros((nb_samples,look_back,num_features))
y_train_reshaped = np.zeros((nb_samples,1,dim_output))
one_hot_labels1 = np.zeros((nb_samples,1,3))
one_hot_label = np.array(pd.get_dummies(labeltrain))


for i in range(nb_samples):
    y_position = i + look_back
    x_train_reshaped1[i] = Xtrain[i:y_position]
    one_hot_labels1[i] = one_hot_label[y_position,:3]
    #one_hot_labels[i] = one_hot_label[y_position-look_back/2,:3]









dataset = np.array(pd.read_excel('alldata0401.xls',header=None))
labels = np.array(dataset[:,72])

dd = dataset.shape
trainl = math.trunc(1.0*dd[0])
test = dd[0]-trainl



for x in range(0, 1):
    dataset0401x =  dataset[:,0:36];#dataset0401x =np.delete(dataset0401x,13,axis=1)
    print(dataset0401x.shape)
    dataset0401y =  dataset[:,36:72];#dataset0401y =np.delete(dataset0401y,13,axis=1)
    acx = np.diff(dataset0401x,axis = 0);acy = np.diff(dataset0401y,axis = 0)
    zz =  np.sqrt(np.power(acx,2) + np.power(acy,2))
    #rx = dataset0401x[:,x]; dataset0401x = np.delete(dataset0401x,(x),axis = 1)
    #ry = dataset0401y[:,x]; dataset0401y = np.delete(dataset0401y,(x),axis = 1)
    rx = np.mean(dataset0401x,axis=1); ry = np.mean(dataset0401y,axis=1);
    rx = np.array([rx])
    rx = rx.reshape(rx.shape[1],1)
    ry = np.array([ry])
    ry = ry.reshape(ry.shape[1],1)
    rx = np.array(dataset0401x)-np.array(rx)
    
    ry = np.array(dataset0401y)-np.array(ry)
    dist = np.sqrt(np.power(rx,2)+ np.power(ry,2))
    dist = np.sort(np.array(dist), axis=1)
    dist = np.array(dist[0:trainl-1,:]);
    #dist = zz
    dist = np.concatenate((dist,zz),axis = 1)  #*changes
    Xtrain = np.array(dist[0:trainl,:]);Xtest = np.array(dist[trainl:,:]);
    labeltrain = np.array(labels[0:trainl],object);
    labeltest = np.array(labels[trainl:]);


nb_samples = Xtrain.shape[0] - look_back
x_train_reshaped = np.zeros((nb_samples,look_back,num_features))
y_train_reshaped = np.zeros((nb_samples,1,dim_output))
one_hot_labels = np.zeros((nb_samples,1,3))
one_hot_label = np.array(pd.get_dummies(labeltrain))


for i in range(nb_samples):
    y_position = i + look_back
    x_train_reshaped[i] = Xtrain[i:y_position]
    one_hot_labels[i] = one_hot_label[y_position,:3]
    #one_hot_labels[i] = one_hot_label[y_position-look_back/2,:3]

print(x_train_reshaped.shape)
"""

# Convolution
kernel_size = 2
filters = 64
#pool_size = 4

model = Sequential()


#model.add(Embedding(max_features, embedding_size, input_length=maxlen))
#model.add(Dropout(0.25))
model.add(Conv1D(filters,
                 kernel_size,
                 padding='same',
                 activation='relu',input_shape = (None,num_features),
                 strides=1))
model.add(MaxPooling1D())

#model.add(Conv1D(64, 4, padding='same', activation='relu'))
#model.add(MaxPooling1D())



model.add(LSTM(n_neurons, input_shape=(None, num_features), activation = 'tanh', return_sequences=True))
model.add(Dropout(0.1))
model.add(TimeDistributed(Dense(3,activation = 'tanh')))
model.add(Activation('softmax'))


model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

model.summary()


filepath="weights-improvement1-{epoch:02d}-{val_acc:.2f}.hdf5"
#checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=2, save_best_only=True, mode='max')
#checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=2, mode='max',save_weights_only=True, period=1)
checkpoint = ModelCheckpoint(filepath, monitor='val_acc',save_best_only=False, save_weights_only=False, mode='auto', period=1)
callbacks_list = [checkpoint]


model.fit(x_train_reshaped, one_hot_labels, epochs=n_epoch, batch_size=n_batch, verbose=2,validation_data=(x_train_reshaped1,one_hot_labels1),callbacks=callbacks_list)


score, acc = model.evaluate(x_train_reshaped1, one_hot_labels1, batch_size=n_batch)
print(score)
print(acc*100)
"""


model = load_model('weights-improvement1-21-0.75.hdf5')
bv = model.predict(x_train_reshaped1)
print(bv.shape)
mali=[]
chj=[]
for j in range(bv.shape[0]):
    bc = (bv[j,:])
    #bc = bc[look_back-23,:]
    bc = bc[look_back-16,:]
    #chj.append(np.argmax(labl[j,:]))
    mali.append(np.argmax(bc))

print(np.array(mali).shape)
laca = laca[30:len(laca)-1];laca = laca.astype(int)
print(np.array(laca))
print(np.array(mali)+1)
ab = confusion_matrix(np.array(laca),(np.array(mali)+1))
print(ab)
print(np.count_nonzero(np.array(chj)-np.array(mali)))
x_train_reshaped1 = x_train_reshaped;one_hot_labels1 = one_hot_labels

















